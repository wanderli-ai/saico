````markdown
# Saico - Simple AI-agent Conversation Orchestrator

`Saico` is a minimal yet powerful JavaScript/Node.js library for managing AI conversations with hierarchical context,
token-aware summarization, and fine-grained control over message flow. Itâ€™s designed to support complex nested 
conversations while maintaining clean summaries and parent context, making it ideal for AI agents, assistants, and 
customer support bots.

---

## âœ¨ Features

- ğŸ“š **Hierarchical Conversations** â€” Track parent-child chat contexts with summary propagation.
- ğŸ§µ **Scoped Memory** â€” Manage sub-conversations independently while maintaining parent relevance.
- ğŸ” **Token-Aware Summarization** â€” Automatically summarize message history based on token thresholds.
- ğŸ’¬ **Message-Level Metadata** â€” Track reply state, summaries, and custom flags.
- ğŸ› ï¸ **OpenAI-Compatible Format** â€” Built for seamless interaction with OpenAI-compatible APIs.
- ğŸ§° **Proxy-Based Interface** â€” Interact with message history like an array, with extra powers.

---

## ğŸ“¦ Installation

```bash
npm install saico-ai-thread --save
````

Or clone manually:

```bash
git clone https://github.com/wanderli-ai/saico
cd saico
```

---

## ğŸ§‘â€ğŸ’» Usage

### Basic Setup

```js
const { createQ } = require('saico');
const openai = require('./openai');

const q = createQ("You are a helpful assistant.", null, "main", 4000);

// Push a message
await q.sendMessage('user', 'Whatâ€™s the weather like tomorrow?', [], {});
```

### Create a Sub-Conversation

```js
const subQ = createQ("Now focus only on the user's hotel bookings.", q, "hotels");

await subQ.sendMessage('user', 'Book me something in Rome.');
await subQ.close(); // Automatically summarizes and passes back to `q`
```

### Hierarchy Example

```text
[Main]
 â””â”€â”€ [hotels] âœ summarized & returned to [Main]
```

---

## ğŸ§  Message API

Each message is stored as:

```js
{
  msg: { role, content, name? },
  opts: { summary?, noreply?, nofunc? },
  msgid: String,
  replied: 0 | 1 | 3
}
```

The conversation history is internally managed and can be accessed via:

* `q[0]` â€” Access nth message
* `q.length` â€” Total messages
* `q.pushSummary(summary)` â€” Manually inject a summary
* `q.getMsgContext()` â€” Get summarized parent chain
* `q.serialize()` â€” Export current state

---

## ğŸ§ª Summary Behavior

Summaries trigger when total token count exceeds 85% of the limit and are always triggered when `close()` is called.
Summaries are:

* Injected as special `[SUMMARY]: ...` messages
* Bubbled up into the parent context
* Excluded from re-summarization unless explicitly kept

Hereâ€™s an updated `README.md` section to document the new Redis integration with observable storage:

---

## ğŸ”„ Redis Integration (Persistent Observable State)

This library includes an optional Redis-based persistence layer to automatically store and update conversation objects (or any JS object) using a **proxy-based observable**.

It supports:

* ğŸ”„ **Auto-saving on change** (with debounce)
* ğŸ§  **Selective serialization** (skips internal/private `_` properties)
* ğŸ—ƒï¸ **Support for serializing `Messages` class**
* ğŸ” **Efficient diff-checking** (saves only when changed)

### ğŸ”§ Setup

1. Install `redis`:

```bash
npm install redis
```

2. Initialize Redis:

```js
const { init, createObservableForRedis } = require('./redis-store'); // adjust path if needed
await init(); // connects to redis://localhost:6379
```

3. Wrap an object to persist changes:

```js
const { createQ } = require('./messages');
const q = createQ("You're a travel assistant.", null, "flights", 3000);

// Wrap with Redis observable
const obsQ = createObservableForRedis("q:session:12345", q);
```

Now, any changes to `obsQ` (e.g., sending messages, updating properties) are **automatically saved** to Redis.

### ğŸ’¡ Use Case: Full User Context

You can also persist a full user session context:

```js
const userContext = {
  userId: 'abc123',
  trip: {},
  q: createQ("Trip assistant", null, "trip", 3000)
};

const observableUser = createObservableForRedis(`user:abc123`, userContext);
```

### ğŸ” Inspecting Last Save

You can retrieve the last save timestamp:

```js
console.log("Last Redis save:", observableUser.lastMod?.());
```

---

## ğŸ§¼ Auto-Sanitization Rules

When saving to Redis:

* All keys starting with `_` are ignored.
* Custom `.serialize()` methods (like on `Messages`) are respected.
* Object updates are **debounced (1s)** and only saved if actual changes are detected.

---

## ğŸ§ª Example Redis Dump (for a `Messages` instance)

```json
{
  "0": { "role": "user", "content": "Whatâ€™s my itinerary?" },
  "1": { "role": "assistant", "content": "Here's your plan..." },
  "lastSave": 1720371212345
}
```
---

## ğŸ”Œ OpenAI Integration

This library expects you to have openai credentials. Support for other model types will be added soon.

* Accepts an array of messages in `{role, content}` format
* Optionally supports function calling

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ saico.js         # Core implementation
â”œâ”€â”€ openai.js        # Openai api wrapper
â”œâ”€â”€ redis.js         # Saico compatible redis wrapper
â”œâ”€â”€ util.js          # Utilities: token counting, etc.
â””â”€â”€ README.md
```

---

## ğŸ” License

MIT License Â© \[Wanderli.ai]

---

## ğŸ™Œ Contributing

Pull requests, issues, and suggestions welcome! Please fork the repo and open a PR, or submit issues directly.

---

## ğŸ“£ Acknowledgements

This project was inspired by the need for a lightweight, non-opinionated alternative to LangChainâ€™s memory modules, 
with full support for real-world LLM conversation flows.

```

---

Let me know if you'd like:
- GitHub Actions CI badge
- NPM publishing support
- Docs site or typedoc config
```

